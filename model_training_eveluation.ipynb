{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/impoxeur2001/Sentiment_Analysis_Darija/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github"}},{"cell_type":"markdown","source":"**NOTEBOOK SETUP**","metadata":{"id":"sormdm-7X0yi"}},{"cell_type":"code","source":"pip install transformers\n","metadata":{"id":"5_gbEPiWO5-N","outputId":"c46d176f-69ad-4e92-e673-7f868d9cf6eb","execution":{"iopub.status.busy":"2023-05-13T16:07:15.660594Z","iopub.execute_input":"2023-05-13T16:07:15.660995Z","iopub.status.idle":"2023-05-13T16:07:26.832960Z","shell.execute_reply.started":"2023-05-13T16:07:15.660964Z","shell.execute_reply":"2023-05-13T16:07:26.831555Z"},"trusted":true},"execution_count":130,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.28.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.64.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.11.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.23.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.3)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.3.23)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.28.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.13.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2.1.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2022.12.7)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install pyarabic","metadata":{"id":"gcLJB8xNUNCM","outputId":"d97a6e9f-0827-425e-b27c-d09ff10dd56e","execution":{"iopub.status.busy":"2023-05-13T16:07:26.836034Z","iopub.execute_input":"2023-05-13T16:07:26.836477Z","iopub.status.idle":"2023-05-13T16:07:38.209228Z","shell.execute_reply.started":"2023-05-13T16:07:26.836418Z","shell.execute_reply":"2023-05-13T16:07:38.207915Z"},"trusted":true},"execution_count":131,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pyarabic in /opt/conda/lib/python3.10/site-packages (0.6.15)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from pyarabic) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from operator import length_hint \nimport torch\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\nfrom sklearn.model_selection import train_test_split\nimport copy\nfrom torch import nn, optim\nfrom torch.utils import data\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup","metadata":{"id":"WD-Wo0E6eD7m","execution":{"iopub.status.busy":"2023-05-13T16:07:38.211541Z","iopub.execute_input":"2023-05-13T16:07:38.211977Z","iopub.status.idle":"2023-05-13T16:07:38.219754Z","shell.execute_reply.started":"2023-05-13T16:07:38.211924Z","shell.execute_reply":"2023-05-13T16:07:38.218758Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":"import codecs\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"id":"-UdshNEtZfP6","execution":{"iopub.status.busy":"2023-05-13T16:07:38.222902Z","iopub.execute_input":"2023-05-13T16:07:38.223615Z","iopub.status.idle":"2023-05-13T16:07:38.229865Z","shell.execute_reply.started":"2023-05-13T16:07:38.223482Z","shell.execute_reply":"2023-05-13T16:07:38.228997Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader","metadata":{"id":"23rPhCQVxDbD","execution":{"iopub.status.busy":"2023-05-13T16:07:38.231480Z","iopub.execute_input":"2023-05-13T16:07:38.231888Z","iopub.status.idle":"2023-05-13T16:07:38.242319Z","shell.execute_reply.started":"2023-05-13T16:07:38.231835Z","shell.execute_reply":"2023-05-13T16:07:38.241287Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"code","source":"from collections import defaultdict","metadata":{"id":"n7vZXeXbM2vg","execution":{"iopub.status.busy":"2023-05-13T16:07:38.244060Z","iopub.execute_input":"2023-05-13T16:07:38.244443Z","iopub.status.idle":"2023-05-13T16:07:38.252197Z","shell.execute_reply.started":"2023-05-13T16:07:38.244365Z","shell.execute_reply":"2023-05-13T16:07:38.251355Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"code","source":"from textwrap import wrap","metadata":{"execution":{"iopub.status.busy":"2023-05-13T16:07:38.255341Z","iopub.execute_input":"2023-05-13T16:07:38.255675Z","iopub.status.idle":"2023-05-13T16:07:38.264087Z","shell.execute_reply.started":"2023-05-13T16:07:38.255640Z","shell.execute_reply":"2023-05-13T16:07:38.263224Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report","metadata":{"execution":{"iopub.status.busy":"2023-05-13T16:07:38.265778Z","iopub.execute_input":"2023-05-13T16:07:38.266461Z","iopub.status.idle":"2023-05-13T16:07:38.274254Z","shell.execute_reply.started":"2023-05-13T16:07:38.266430Z","shell.execute_reply":"2023-05-13T16:07:38.273348Z"},"trusted":true},"execution_count":137,"outputs":[]},{"cell_type":"code","source":"RANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","metadata":{"id":"FoIOPXi-3Cwf","execution":{"iopub.status.busy":"2023-05-13T16:07:38.275832Z","iopub.execute_input":"2023-05-13T16:07:38.276496Z","iopub.status.idle":"2023-05-13T16:07:38.286948Z","shell.execute_reply.started":"2023-05-13T16:07:38.276464Z","shell.execute_reply":"2023-05-13T16:07:38.286100Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"code","source":"pip install aaransia","metadata":{"id":"R3mkNzoZxhX1","outputId":"9cb2d757-3e4a-4f6d-bc4d-1f36701135c9","execution":{"iopub.status.busy":"2023-05-13T16:07:38.291961Z","iopub.execute_input":"2023-05-13T16:07:38.292814Z","iopub.status.idle":"2023-05-13T16:07:49.285718Z","shell.execute_reply.started":"2023-05-13T16:07:38.292777Z","shell.execute_reply":"2023-05-13T16:07:49.284374Z"},"trusted":true},"execution_count":139,"outputs":[{"name":"stdout","text":"Requirement already satisfied: aaransia in /opt/conda/lib/python3.10/site-packages (1.1)\nRequirement already satisfied: unidecode in /opt/conda/lib/python3.10/site-packages (from aaransia) (1.3.6)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from aaransia) (1.23.5)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from aaransia) (1.5.3)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->aaransia) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->aaransia) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->aaransia) (1.16.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"from aaransia import transliterate \n","metadata":{"id":"ytdf6JRmxyKH","execution":{"iopub.status.busy":"2023-05-13T16:07:49.288157Z","iopub.execute_input":"2023-05-13T16:07:49.288841Z","iopub.status.idle":"2023-05-13T16:07:49.296388Z","shell.execute_reply.started":"2023-05-13T16:07:49.288791Z","shell.execute_reply":"2023-05-13T16:07:49.293889Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"code","source":"import string\nimport re\nfrom pyarabic.araby import strip_tatweel,strip_tashkeel,normalize_ligature","metadata":{"id":"pPMEQElPU-8v","execution":{"iopub.status.busy":"2023-05-13T16:07:49.298827Z","iopub.execute_input":"2023-05-13T16:07:49.299677Z","iopub.status.idle":"2023-05-13T16:07:49.306242Z","shell.execute_reply.started":"2023-05-13T16:07:49.299630Z","shell.execute_reply":"2023-05-13T16:07:49.305184Z"},"trusted":true},"execution_count":141,"outputs":[]},{"cell_type":"markdown","source":"**DATA EXPLORATION**","metadata":{"id":"X1TXZ1V7Xi9w"}},{"cell_type":"code","source":"df= pd.read_csv('/kaggle/input/dataset-darija-1/dataset_darija_1.csv', on_bad_lines='skip', sep = ';')\ndf.columns = ['sentiment','sentence']\n##df_2= pd.read_csv('/kaggle/input/dataset/dataset_darija_3.csv', on_bad_lines='skip', sep = ';',index_col=None)\n#df_2.columns = ['sentiment','sentence']\n#df=pd.concat([df_1, df_2], ignore_index=True, sort=False)\n\ndf['sentiment'] = df['sentiment'].str.replace(\" \", \"\")\ndf['sentiment'] = df['sentiment'].str.rstrip()","metadata":{"id":"Tq1zk8RvnsNZ","execution":{"iopub.status.busy":"2023-05-13T16:07:49.308388Z","iopub.execute_input":"2023-05-13T16:07:49.308767Z","iopub.status.idle":"2023-05-13T16:07:49.337632Z","shell.execute_reply.started":"2023-05-13T16:07:49.308736Z","shell.execute_reply":"2023-05-13T16:07:49.336739Z"},"trusted":true},"execution_count":142,"outputs":[]},{"cell_type":"code","source":"df.tail()\n","metadata":{"execution":{"iopub.status.busy":"2023-05-13T16:07:49.339124Z","iopub.execute_input":"2023-05-13T16:07:49.339492Z","iopub.status.idle":"2023-05-13T16:07:49.355114Z","shell.execute_reply.started":"2023-05-13T16:07:49.339450Z","shell.execute_reply":"2023-05-13T16:07:49.353891Z"},"trusted":true},"execution_count":143,"outputs":[{"execution_count":143,"output_type":"execute_result","data":{"text/plain":"     sentiment                                           sentence\n2048       neg                   اصمت لعلى صمتك راحة بالنسبة لهم \n2049       neg   حديقة حيوانات و لازال هنالك اناس لا يؤمنون بن...\n2050       neg   أفعى بجدارة تريثت تربصت و كان الفحيح متعة له ...\n2051       neg   لا يقطع الرأس غير الي ركبه الان اصبح تركيب ال...\n2052       neg               امة النون نستنكر ندين نشجب ثم نوافق ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2048</th>\n      <td>neg</td>\n      <td>اصمت لعلى صمتك راحة بالنسبة لهم</td>\n    </tr>\n    <tr>\n      <th>2049</th>\n      <td>neg</td>\n      <td>حديقة حيوانات و لازال هنالك اناس لا يؤمنون بن...</td>\n    </tr>\n    <tr>\n      <th>2050</th>\n      <td>neg</td>\n      <td>أفعى بجدارة تريثت تربصت و كان الفحيح متعة له ...</td>\n    </tr>\n    <tr>\n      <th>2051</th>\n      <td>neg</td>\n      <td>لا يقطع الرأس غير الي ركبه الان اصبح تركيب ال...</td>\n    </tr>\n    <tr>\n      <th>2052</th>\n      <td>neg</td>\n      <td>امة النون نستنكر ندين نشجب ثم نوافق</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"le=LabelEncoder()\ndf['sentiment']= le.fit_transform(df['sentiment'])\ndf=df[df['sentiment']!=2]","metadata":{"id":"ZdChFlXKVPe_","execution":{"iopub.status.busy":"2023-05-13T16:07:49.356905Z","iopub.execute_input":"2023-05-13T16:07:49.357192Z","iopub.status.idle":"2023-05-13T16:07:49.367904Z","shell.execute_reply.started":"2023-05-13T16:07:49.357167Z","shell.execute_reply":"2023-05-13T16:07:49.366773Z"},"trusted":true},"execution_count":144,"outputs":[]},{"cell_type":"code","source":"df['sentiment'].value_counts()","metadata":{"id":"RH7j93JWKc-O","outputId":"a71a034c-2ccd-45af-856b-39bf9b08d3c2","execution":{"iopub.status.busy":"2023-05-13T16:07:49.369581Z","iopub.execute_input":"2023-05-13T16:07:49.369886Z","iopub.status.idle":"2023-05-13T16:07:49.381389Z","shell.execute_reply.started":"2023-05-13T16:07:49.369834Z","shell.execute_reply":"2023-05-13T16:07:49.380149Z"},"trusted":true},"execution_count":145,"outputs":[{"execution_count":145,"output_type":"execute_result","data":{"text/plain":"1    1031\n0    1021\nName: sentiment, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"sns.countplot(df, x='sentiment')\nplt.xlabel('sentiment')","metadata":{"id":"V95zLjaStMML","outputId":"5329cc6c-327f-4032-c766-5ac5846da597","execution":{"iopub.status.busy":"2023-05-13T16:07:49.383424Z","iopub.execute_input":"2023-05-13T16:07:49.383804Z","iopub.status.idle":"2023-05-13T16:07:49.582651Z","shell.execute_reply.started":"2023-05-13T16:07:49.383771Z","shell.execute_reply":"2023-05-13T16:07:49.581744Z"},"trusted":true},"execution_count":146,"outputs":[{"execution_count":146,"output_type":"execute_result","data":{"text/plain":"Text(0.5, 0, 'sentiment')"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"class_values=[0,1]","metadata":{"id":"CbE5krsML4Zv","execution":{"iopub.status.busy":"2023-05-13T16:07:49.584120Z","iopub.execute_input":"2023-05-13T16:07:49.584434Z","iopub.status.idle":"2023-05-13T16:07:49.589457Z","shell.execute_reply.started":"2023-05-13T16:07:49.584404Z","shell.execute_reply":"2023-05-13T16:07:49.588508Z"},"trusted":true},"execution_count":147,"outputs":[]},{"cell_type":"markdown","source":"**PREPROCESSING**","metadata":{"id":"bPqnEt56WTWV"}},{"cell_type":"code","source":"\nDarijaBERT_tokenizer = AutoTokenizer.from_pretrained(\"SI2M-Lab/DarijaBERT\")\nDarijaBert_model = AutoModel.from_pretrained(\"SI2M-Lab/DarijaBERT\")","metadata":{"id":"XcCrkdASP1xp","outputId":"c9f1ad7c-556a-416f-ed63-2d966dd15881","execution":{"iopub.status.busy":"2023-05-13T16:07:49.590729Z","iopub.execute_input":"2023-05-13T16:07:49.591599Z","iopub.status.idle":"2023-05-13T16:07:52.103106Z","shell.execute_reply.started":"2023-05-13T16:07:49.591567Z","shell.execute_reply":"2023-05-13T16:07:52.102136Z"},"trusted":true},"execution_count":148,"outputs":[{"name":"stderr","text":"The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\nThe `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\nThe `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\nSome weights of the model checkpoint at SI2M-Lab/DarijaBERT were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"def repted(text):\n    text=re.sub(r'(.)\\1+', r'\\1', text)# Replace with only one (remove repetitions)  \n    return text","metadata":{"id":"XVhyq3gfVtP6","execution":{"iopub.status.busy":"2023-05-13T16:07:52.104983Z","iopub.execute_input":"2023-05-13T16:07:52.105358Z","iopub.status.idle":"2023-05-13T16:07:52.112651Z","shell.execute_reply.started":"2023-05-13T16:07:52.105325Z","shell.execute_reply":"2023-05-13T16:07:52.111615Z"},"trusted":true},"execution_count":149,"outputs":[]},{"cell_type":"code","source":"def get_emojis():\n    with codecs.open(\"emoji.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as myfile:\n         positive_emoji=myfile.read()\n    positive_emoji=positive_emoji.split(\"\\r\\n\")\n    positive_emoji=positive_emoji[1:len(positive_emoji)-1] \n    with codecs.open(\"neg_emoji.txt\", \"r\", encoding=\"utf-8\", errors=\"ignore\") as myfie:\n        neg_emoji=myfie.read()\n    neg_emoji=neg_emoji.split(\"\\r\\n\")\n    neg_emoji=neg_emoji[1:len(neg_emoji)-1]\n    return positive_emoji,neg_emoji","metadata":{"id":"Bg5XoBaQZq_a","execution":{"iopub.status.busy":"2023-05-13T16:07:52.115695Z","iopub.execute_input":"2023-05-13T16:07:52.116643Z","iopub.status.idle":"2023-05-13T16:07:52.125751Z","shell.execute_reply.started":"2023-05-13T16:07:52.116608Z","shell.execute_reply":"2023-05-13T16:07:52.124801Z"},"trusted":true},"execution_count":150,"outputs":[]},{"cell_type":"code","source":"def emoji_to_text(text):\n    text_words = []\n    words = text.split(\" \")\n    positive_emoji,neg_emoji=get_emojis()\n    for word in words:\n        if emoji_pattern.search(word):\n            if word in positive_emoji :\n                word='ايجابي'\n            if word in neg_emoji :\n                word='سلبي'\n        text_words.append(word)\n    return ' '.join(text_words)","metadata":{"id":"JyduZn4EXUvB","execution":{"iopub.status.busy":"2023-05-13T16:07:52.128874Z","iopub.execute_input":"2023-05-13T16:07:52.129152Z","iopub.status.idle":"2023-05-13T16:07:52.137304Z","shell.execute_reply.started":"2023-05-13T16:07:52.129128Z","shell.execute_reply":"2023-05-13T16:07:52.136364Z"},"trusted":true},"execution_count":151,"outputs":[]},{"cell_type":"code","source":"def remove_emojis(data):\n    emoj = re.compile(\"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n        u\"\\U00002500-\\U00002BEF\"  # chinese char\n        u\"\\U00002702-\\U000027B0\"\n        u\"\\U00002702-\\U000027B0\"\n        u\"\\U000024C2-\\U0001F251\"\n        u\"\\U0001f926-\\U0001f937\"\n        u\"\\U00010000-\\U0010ffff\"\n        u\"\\u2640-\\u2642\" \n        u\"\\u2600-\\u2B55\"\n        u\"\\u200d\"\n        u\"\\u23cf\"\n        u\"\\u23e9\"\n        u\"\\u231a\"\n        u\"\\ufe0f\"  # dingbats\n        u\"\\u3030\"\n                      \"]+\", re.UNICODE)\n    return re.sub(emoj, '', data)","metadata":{"id":"mvftV-8c1iNY","execution":{"iopub.status.busy":"2023-05-13T16:07:52.138879Z","iopub.execute_input":"2023-05-13T16:07:52.139299Z","iopub.status.idle":"2023-05-13T16:07:52.146736Z","shell.execute_reply.started":"2023-05-13T16:07:52.139268Z","shell.execute_reply":"2023-05-13T16:07:52.145819Z"},"trusted":true},"execution_count":152,"outputs":[]},{"cell_type":"code","source":"def strip_links(text):\n    link_regex    = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n    links         = re.findall(link_regex, text)\n    for link in links:\n        text = text.replace(link[0], ', ')\n    return text","metadata":{"id":"qprNktz3Kxb2","execution":{"iopub.status.busy":"2023-05-13T16:07:52.148232Z","iopub.execute_input":"2023-05-13T16:07:52.148566Z","iopub.status.idle":"2023-05-13T16:07:52.157619Z","shell.execute_reply.started":"2023-05-13T16:07:52.148535Z","shell.execute_reply":"2023-05-13T16:07:52.156699Z"},"trusted":true},"execution_count":153,"outputs":[]},{"cell_type":"code","source":"def pre_processing(df,source,field):\n    df[field] = df[source]\n    df[field] = strip_links(df[field]) # remove links\n    df[field] = emoji_to_text(df[field]) # replace emojies with their sentiments\n    df[field] = df[field].replace(r'@[^\\s]+', 'USER', regex=True) # Replace user mentions with USER string\n    df[field] = df[field].replace(r'#[^\\s]+', 'HASHTAG', regex=True) # Replace Hashtags with HASHTAG string\n    df=df[df[field].apply(lambda x:len(re.findall(r'[\\u0600-\\u06FF]+', x)))>1] #Keep sequences with at least 2 arabic words\n    df[field] = df[field].apply(strip_tatweel) #Remove Tatweel string \n    df[field] = df[field].apply(strip_tashkeel) # Remove Diacritics\n    df[field] = df[field].apply(repted)\n    return df","metadata":{"id":"gjkLhvHKWQYC","execution":{"iopub.status.busy":"2023-05-13T16:07:52.158683Z","iopub.execute_input":"2023-05-13T16:07:52.158987Z","iopub.status.idle":"2023-05-13T16:07:52.169388Z","shell.execute_reply.started":"2023-05-13T16:07:52.158962Z","shell.execute_reply":"2023-05-13T16:07:52.168268Z"},"trusted":true},"execution_count":154,"outputs":[]},{"cell_type":"code","source":"sentence_1=[]\nfor sentence in df['sentence']:\n    sentence= strip_tatweel(sentence)\n    sentence= normalize_ligature(sentence)\n    sentence= repted(sentence)\n    sentence_1.append(sentence)\ndf.drop(['sentence'], axis=1)   \ndf.assign(sentence=sentence_1)","metadata":{"execution":{"iopub.status.busy":"2023-05-13T16:07:52.170887Z","iopub.execute_input":"2023-05-13T16:07:52.171438Z","iopub.status.idle":"2023-05-13T16:07:52.226942Z","shell.execute_reply.started":"2023-05-13T16:07:52.171396Z","shell.execute_reply":"2023-05-13T16:07:52.225910Z"},"trusted":true},"execution_count":155,"outputs":[{"execution_count":155,"output_type":"execute_result","data":{"text/plain":"      sentiment                                           sentence\n0             1   طوال حياتي لم المس اي تغير حتى قدمت هذه الحكو...\n1             1  هاذي بلاد الشرفاء و بلاد رجال لي دافعو على أرض...\n2             1            كل التوفيق بدر هاري ولد الشعب راك عزيز \n3             1                              كاتعجبني هاد أغنية 😎 \n4             1              يتبرع باموال باهضة لفقراء فخر المغرب \n...         ...                                                ...\n2048          0                   اصمت لعلى صمتك راحة بالنسبة لهم \n2049          0   حديقة حيوانات و لازال هنالك اناس لا يؤمنون بن...\n2050          0   أفعى بجدارة تريثت تربصت و كان الفحيح متعة له ...\n2051          0   لا يقطع الرأس غير الي ركبه الان اصبح تركيب ال...\n2052          0               امة النون نستنكر ندين نشجب ثم نوافق \n\n[2052 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>طوال حياتي لم المس اي تغير حتى قدمت هذه الحكو...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>هاذي بلاد الشرفاء و بلاد رجال لي دافعو على أرض...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>كل التوفيق بدر هاري ولد الشعب راك عزيز</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>كاتعجبني هاد أغنية 😎</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>يتبرع باموال باهضة لفقراء فخر المغرب</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2048</th>\n      <td>0</td>\n      <td>اصمت لعلى صمتك راحة بالنسبة لهم</td>\n    </tr>\n    <tr>\n      <th>2049</th>\n      <td>0</td>\n      <td>حديقة حيوانات و لازال هنالك اناس لا يؤمنون بن...</td>\n    </tr>\n    <tr>\n      <th>2050</th>\n      <td>0</td>\n      <td>أفعى بجدارة تريثت تربصت و كان الفحيح متعة له ...</td>\n    </tr>\n    <tr>\n      <th>2051</th>\n      <td>0</td>\n      <td>لا يقطع الرأس غير الي ركبه الان اصبح تركيب ال...</td>\n    </tr>\n    <tr>\n      <th>2052</th>\n      <td>0</td>\n      <td>امة النون نستنكر ندين نشجب ثم نوافق</td>\n    </tr>\n  </tbody>\n</table>\n<p>2052 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"sample_txt= \"ch7al zwina dnia\"\nsample_txt_ar=transliterate(sample_txt, source='ma', target='ar', universal=True)\ntokens = DarijaBERT_tokenizer.tokenize(sample_txt_ar)\ntoken_ids = DarijaBERT_tokenizer.convert_tokens_to_ids(tokens)\nprint(f' Sentence: {sample_txt_ar}')\nprint(f'   Tokens: {tokens}')\nprint(f'Token IDs: {token_ids}')","metadata":{"id":"2WcewrF2uMt7","outputId":"f6d1dd52-3cb3-4ef0-9f1b-0456104fcde0","execution":{"iopub.status.busy":"2023-05-13T16:07:52.228232Z","iopub.execute_input":"2023-05-13T16:07:52.228555Z","iopub.status.idle":"2023-05-13T16:07:52.236588Z","shell.execute_reply.started":"2023-05-13T16:07:52.228525Z","shell.execute_reply":"2023-05-13T16:07:52.235197Z"},"trusted":true},"execution_count":156,"outputs":[{"name":"stdout","text":" Sentence: شحال زوينا دنيا\n   Tokens: ['شحال', 'زوينا', 'دنيا']\nToken IDs: [2448, 22504, 4236]\n","output_type":"stream"}]},{"cell_type":"code","source":"encoding = DarijaBERT_tokenizer.encode_plus(\n  sample_txt,\n  max_length=32,\n  truncation=True,\n  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n  return_token_type_ids=False,\n  pad_to_max_length=True,\n  return_attention_mask=True,\n  return_tensors='pt',  # Return PyTorch tensors\n)\nencoding.keys()","metadata":{"id":"XSqLkzLY4cm6","outputId":"d2014c09-6293-43dd-cd19-ccabe70d5793","execution":{"iopub.status.busy":"2023-05-13T16:07:52.245790Z","iopub.execute_input":"2023-05-13T16:07:52.247175Z","iopub.status.idle":"2023-05-13T16:07:52.256345Z","shell.execute_reply.started":"2023-05-13T16:07:52.247142Z","shell.execute_reply":"2023-05-13T16:07:52.255230Z"},"trusted":true},"execution_count":157,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"},{"execution_count":157,"output_type":"execute_result","data":{"text/plain":"dict_keys(['input_ids', 'attention_mask'])"},"metadata":{}}]},{"cell_type":"code","source":"encoding['input_ids']","metadata":{"id":"bXf9-iP1Bxrd","outputId":"cd61208d-50b4-4137-d661-e3b1a9512ce4","execution":{"iopub.status.busy":"2023-05-13T16:07:52.257863Z","iopub.execute_input":"2023-05-13T16:07:52.258315Z","iopub.status.idle":"2023-05-13T16:07:52.266966Z","shell.execute_reply.started":"2023-05-13T16:07:52.258283Z","shell.execute_reply":"2023-05-13T16:07:52.265747Z"},"trusted":true},"execution_count":158,"outputs":[{"execution_count":158,"output_type":"execute_result","data":{"text/plain":"tensor([[    2,  8094,  1041,  3630, 56472,    48, 19829,  1058,     3,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0]])"},"metadata":{}}]},{"cell_type":"code","source":"token_lens=[]\n\nfor txt in df.sentence:\n  encodings=DarijaBERT_tokenizer.encode(txt,max_length=512)\n  var=length_hint(encodings)\n  token_lens.append(var)\nsns.displot(token_lens)  ","metadata":{"id":"7HkSpvq-e-sh","outputId":"f1a195a0-3387-47f8-a338-72cb0397f254","execution":{"iopub.status.busy":"2023-05-13T16:07:52.268967Z","iopub.execute_input":"2023-05-13T16:07:52.269428Z","iopub.status.idle":"2023-05-13T16:07:53.004998Z","shell.execute_reply.started":"2023-05-13T16:07:52.269397Z","shell.execute_reply":"2023-05-13T16:07:53.004112Z"},"trusted":true},"execution_count":159,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"execution_count":159,"output_type":"execute_result","data":{"text/plain":"<seaborn.axisgrid.FacetGrid at 0x790d569883d0>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 500x500 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"**Creat a Pytorch Dataset**","metadata":{"id":"7O1DeyxF5jmd"}},{"cell_type":"code","source":"class GPSentenceDataset(Dataset):\n  def __init__(self, sentences, targets, tokenizer, max_len):\n    self.sentences = sentences\n    self.targets = targets\n    self.tokenizer = tokenizer\n    self.max_len = max_len\n  def __len__(self):\n    return len(self.sentences)\n  def __getitem__(self, item):\n    sentence = str(self.sentences[item])\n    target = self.targets[item]\n    encoding = self.tokenizer.encode_plus(\n      sentence,\n      add_special_tokens=True,\n      max_length=self.max_len,\n      return_token_type_ids=False,\n      pad_to_max_length=True,\n      return_attention_mask=True,\n      return_tensors='pt',\n    )\n    input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n    attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n    return {\n      'sentence_text': sentence,\n      'input_ids': encoding['input_ids'].flatten(),\n      'attention_mask': encoding['attention_mask'].flatten(),\n      'targets': torch.tensor(target, dtype=torch.long)\n    }","metadata":{"id":"-5YS2QvNtvsf","execution":{"iopub.status.busy":"2023-05-13T16:07:53.006806Z","iopub.execute_input":"2023-05-13T16:07:53.007181Z","iopub.status.idle":"2023-05-13T16:07:53.016024Z","shell.execute_reply.started":"2023-05-13T16:07:53.007147Z","shell.execute_reply":"2023-05-13T16:07:53.014911Z"},"trusted":true},"execution_count":160,"outputs":[]},{"cell_type":"code","source":"for sentence in df.sentence:\n  sentence= remove_emojis(sentence)","metadata":{"id":"hW0SnqAExK4X","execution":{"iopub.status.busy":"2023-05-13T16:07:53.017756Z","iopub.execute_input":"2023-05-13T16:07:53.018160Z","iopub.status.idle":"2023-05-13T16:07:53.050664Z","shell.execute_reply.started":"2023-05-13T16:07:53.018124Z","shell.execute_reply":"2023-05-13T16:07:53.049768Z"},"trusted":true},"execution_count":161,"outputs":[]},{"cell_type":"markdown","source":"**SPLIT THE DATASET**\n","metadata":{"id":"vtJEk7YD2M-4"}},{"cell_type":"code","source":"df_train, df_test = train_test_split(\n  df,\n  test_size=0.1,\n  random_state=RANDOM_SEED\n)\ndf_val, df_test = train_test_split(\n  df_test,\n  test_size=0.5,\n  random_state=RANDOM_SEED\n)","metadata":{"id":"9MJBXhUq2Rwa","execution":{"iopub.status.busy":"2023-05-13T16:07:53.052987Z","iopub.execute_input":"2023-05-13T16:07:53.053635Z","iopub.status.idle":"2023-05-13T16:07:53.061332Z","shell.execute_reply.started":"2023-05-13T16:07:53.053599Z","shell.execute_reply":"2023-05-13T16:07:53.060415Z"},"trusted":true},"execution_count":162,"outputs":[]},{"cell_type":"markdown","source":"**Creat Data Loaders**","metadata":{"id":"Atptran15zRZ"}},{"cell_type":"code","source":"","metadata":{"id":"j6Q8j2kfdgim"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_data_loader(df, tokenizer, max_len, batch_size):\n  ds = GPSentenceDataset(\n    sentences=df.sentence.to_numpy(),\n    targets=df.sentiment.to_numpy(),\n    tokenizer=tokenizer,\n    max_len=max_len\n  )\n  return DataLoader(\n    ds,\n    batch_size=batch_size,\n    num_workers=1\n  )","metadata":{"id":"YqMZggpm54IR","execution":{"iopub.status.busy":"2023-05-13T16:07:53.063046Z","iopub.execute_input":"2023-05-13T16:07:53.063439Z","iopub.status.idle":"2023-05-13T16:07:53.069787Z","shell.execute_reply.started":"2023-05-13T16:07:53.063405Z","shell.execute_reply":"2023-05-13T16:07:53.068638Z"},"trusted":true},"execution_count":163,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 16\nMAX_LEN = 180\ntrain_data_loader = create_data_loader(df_train, DarijaBERT_tokenizer, MAX_LEN, BATCH_SIZE)\nval_data_loader = create_data_loader(df_val, DarijaBERT_tokenizer, MAX_LEN, BATCH_SIZE)\ntest_data_loader = create_data_loader(df_test, DarijaBERT_tokenizer, MAX_LEN, BATCH_SIZE)","metadata":{"id":"mKY7cT_Y6Sto","execution":{"iopub.status.busy":"2023-05-13T16:07:53.071531Z","iopub.execute_input":"2023-05-13T16:07:53.071996Z","iopub.status.idle":"2023-05-13T16:07:53.080421Z","shell.execute_reply.started":"2023-05-13T16:07:53.071958Z","shell.execute_reply":"2023-05-13T16:07:53.079410Z"},"trusted":true},"execution_count":164,"outputs":[]},{"cell_type":"code","source":"ds = GPSentenceDataset(\n    sentences=df.sentence.to_numpy(),\n    targets=df.sentiment.to_numpy(),\n    tokenizer=DarijaBERT_tokenizer,\n    max_len=MAX_LEN\n  )\n","metadata":{"id":"zBqlD8RT9iWe","execution":{"iopub.status.busy":"2023-05-13T16:07:53.083008Z","iopub.execute_input":"2023-05-13T16:07:53.083815Z","iopub.status.idle":"2023-05-13T16:07:53.091639Z","shell.execute_reply.started":"2023-05-13T16:07:53.083750Z","shell.execute_reply":"2023-05-13T16:07:53.090440Z"},"trusted":true},"execution_count":165,"outputs":[]},{"cell_type":"code","source":"df.dtypes\n","metadata":{"id":"eoT6ob5uGH6L","outputId":"f612b4ab-aabc-439b-aa9d-abbefc9c44e5","execution":{"iopub.status.busy":"2023-05-13T16:07:53.093320Z","iopub.execute_input":"2023-05-13T16:07:53.093975Z","iopub.status.idle":"2023-05-13T16:07:53.104716Z","shell.execute_reply.started":"2023-05-13T16:07:53.093933Z","shell.execute_reply":"2023-05-13T16:07:53.103805Z"},"trusted":true},"execution_count":166,"outputs":[{"execution_count":166,"output_type":"execute_result","data":{"text/plain":"sentiment     int64\nsentence     object\ndtype: object"},"metadata":{}}]},{"cell_type":"code","source":"data = next(iter(train_data_loader))\ndata.keys()\n","metadata":{"id":"nuXXfpqnCjm_","outputId":"b0b43aeb-bf9d-434c-bdd0-ef10678419a1","execution":{"iopub.status.busy":"2023-05-13T16:07:53.106063Z","iopub.execute_input":"2023-05-13T16:07:53.106407Z","iopub.status.idle":"2023-05-13T16:07:53.308128Z","shell.execute_reply.started":"2023-05-13T16:07:53.106376Z","shell.execute_reply":"2023-05-13T16:07:53.307032Z"},"trusted":true},"execution_count":167,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"execution_count":167,"output_type":"execute_result","data":{"text/plain":"dict_keys(['sentence_text', 'input_ids', 'attention_mask', 'targets'])"},"metadata":{}}]},{"cell_type":"code","source":"print(data['input_ids'].shape)\nprint(data['attention_mask'].shape)\nprint(data['targets'].shape)","metadata":{"id":"4m8g9ccMjWx7","outputId":"a1f3aeb7-d660-4d02-9e76-27e582dc277e","execution":{"iopub.status.busy":"2023-05-13T16:07:53.310283Z","iopub.execute_input":"2023-05-13T16:07:53.310711Z","iopub.status.idle":"2023-05-13T16:07:53.318758Z","shell.execute_reply.started":"2023-05-13T16:07:53.310664Z","shell.execute_reply":"2023-05-13T16:07:53.317528Z"},"trusted":true},"execution_count":168,"outputs":[{"name":"stdout","text":"torch.Size([16, 180])\ntorch.Size([16, 180])\ntorch.Size([16])\n","output_type":"stream"}]},{"cell_type":"code","source":"last_hidden_state, pooled_output = DarijaBert_model(\n  input_ids=encoding['input_ids'],\n  attention_mask=encoding['attention_mask']\n)","metadata":{"id":"wf4ZwPXTncB2","execution":{"iopub.status.busy":"2023-05-13T16:07:53.320393Z","iopub.execute_input":"2023-05-13T16:07:53.321119Z","iopub.status.idle":"2023-05-13T16:07:53.465718Z","shell.execute_reply.started":"2023-05-13T16:07:53.321086Z","shell.execute_reply":"2023-05-13T16:07:53.464555Z"},"trusted":true},"execution_count":169,"outputs":[]},{"cell_type":"code","source":"DarijaBert_model.config\n","metadata":{"id":"bTo5lMwEpnnS","outputId":"ce60ea33-fcdc-4852-eb88-b19aafbd86e9","execution":{"iopub.status.busy":"2023-05-13T16:07:53.467628Z","iopub.execute_input":"2023-05-13T16:07:53.468568Z","iopub.status.idle":"2023-05-13T16:07:53.476658Z","shell.execute_reply.started":"2023-05-13T16:07:53.468536Z","shell.execute_reply":"2023-05-13T16:07:53.475674Z"},"trusted":true},"execution_count":170,"outputs":[{"execution_count":170,"output_type":"execute_result","data":{"text/plain":"BertConfig {\n  \"_name_or_path\": \"SI2M-Lab/DarijaBERT\",\n  \"architectures\": [\n    \"BertForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"classifier_dropout\": null,\n  \"cls_token\": \"[CLS]\",\n  \"do_lower_case\": true,\n  \"gradient_checkpointing\": false,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.1,\n  \"hidden_size\": 768,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 3072,\n  \"layer_norm_eps\": 1e-12,\n  \"mask_token\": \"[MASK]\",\n  \"max_len\": 128,\n  \"max_position_embeddings\": 512,\n  \"model_max_length\": 128,\n  \"model_type\": \"bert\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token\": \"[PAD]\",\n  \"pad_token_id\": 0,\n  \"position_embedding_type\": \"absolute\",\n  \"sep_token\": \"[SEP]\",\n  \"transformers_version\": \"4.28.1\",\n  \"type_vocab_size\": 2,\n  \"unk_token\": \"[UNK]\",\n  \"use_cache\": true,\n  \"vocab_size\": 80000\n}"},"metadata":{}}]},{"cell_type":"markdown","source":"**Building a sentiment classifier**","metadata":{"id":"hA-BjxX6stBv"}},{"cell_type":"code","source":"class SentimentClassifier(nn.Module):\n  def __init__(self, n_classes):\n    super(SentimentClassifier, self).__init__()\n    self.bert = DarijaBert_model = AutoModel.from_pretrained(\"SI2M-Lab/DarijaBERT\", return_dict=False)\n    self.drop = nn.Dropout(p=0.3)\n    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n    self.softmax = nn.Softmax(dim=1)\n\n  def forward(self, input_ids, attention_mask):\n    _, pooled_output = self.bert(\n      input_ids=input_ids,\n      attention_mask=attention_mask\n    )\n    output = self.drop(pooled_output)\n    output = self.out(output)\n    return self.softmax(output)","metadata":{"id":"Pu3w1IKnst7Z","execution":{"iopub.status.busy":"2023-05-13T16:07:53.478204Z","iopub.execute_input":"2023-05-13T16:07:53.479274Z","iopub.status.idle":"2023-05-13T16:07:53.488904Z","shell.execute_reply.started":"2023-05-13T16:07:53.479239Z","shell.execute_reply":"2023-05-13T16:07:53.487746Z"},"trusted":true},"execution_count":171,"outputs":[]},{"cell_type":"code","source":"model = SentimentClassifier(len(class_values))\nmodel = model.to(device)","metadata":{"id":"l5U78sA7KplG","outputId":"d0810213-e759-4f51-909f-b0fb22ad8bf6","execution":{"iopub.status.busy":"2023-05-13T16:07:53.490412Z","iopub.execute_input":"2023-05-13T16:07:53.490785Z","iopub.status.idle":"2023-05-13T16:07:57.568550Z","shell.execute_reply.started":"2023-05-13T16:07:53.490749Z","shell.execute_reply":"2023-05-13T16:07:57.567529Z"},"trusted":true},"execution_count":172,"outputs":[{"name":"stderr","text":"The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\nSome weights of the model checkpoint at SI2M-Lab/DarijaBERT were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"input_ids = data['input_ids'].to(device)\nattention_mask = data['attention_mask'].to(device)\nprint(input_ids.shape) # batch size x seq length\nprint(attention_mask.shape) # batch size x seq length","metadata":{"id":"KhcFx02hMKKr","outputId":"9610d38f-4929-47c0-b18e-94f49ab4d8c2","execution":{"iopub.status.busy":"2023-05-13T16:07:57.569979Z","iopub.execute_input":"2023-05-13T16:07:57.570365Z","iopub.status.idle":"2023-05-13T16:07:57.578736Z","shell.execute_reply.started":"2023-05-13T16:07:57.570328Z","shell.execute_reply":"2023-05-13T16:07:57.577677Z"},"trusted":true},"execution_count":173,"outputs":[{"name":"stdout","text":"torch.Size([16, 180])\ntorch.Size([16, 180])\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Training**","metadata":{}},{"cell_type":"code","source":"EPOCHS = 30\noptimizer = AdamW(model.parameters(), lr=1e-8, correct_bias=False)\ntotal_steps = len(train_data_loader) * EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n  optimizer,\n  num_warmup_steps=0,\n  num_training_steps=total_steps\n)\nloss_fn = nn.CrossEntropyLoss().to(device)","metadata":{"execution":{"iopub.status.busy":"2023-05-13T16:07:57.580222Z","iopub.execute_input":"2023-05-13T16:07:57.580744Z","iopub.status.idle":"2023-05-13T16:07:57.592678Z","shell.execute_reply.started":"2023-05-13T16:07:57.580711Z","shell.execute_reply":"2023-05-13T16:07:57.591573Z"},"trusted":true},"execution_count":174,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"def train_epoch(\n  model,\n  data_loader,\n  loss_fn,\n  optimizer,\n  device,\n  scheduler,\n  n_examples\n):\n  model = model.train()\n\n  losses = []\n  correct_predictions = 0\n\n  for d in data_loader:\n    input_ids = d[\"input_ids\"].to(device)\n    attention_mask = d[\"attention_mask\"].to(device)\n    targets = d[\"targets\"].to(device)\n\n    outputs = model(\n      input_ids=input_ids,\n      attention_mask=attention_mask\n    )\n\n    _, preds = torch.max(outputs, dim=1)\n    loss = loss_fn(outputs, targets)\n\n    correct_predictions += torch.sum(preds == targets)\n    losses.append(loss.item())\n\n    loss.backward()\n    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n    optimizer.step()\n    scheduler.step()\n    optimizer.zero_grad()\n    \n  return correct_predictions.double() / n_examples, np.mean(losses)","metadata":{"execution":{"iopub.status.busy":"2023-05-13T16:07:57.594304Z","iopub.execute_input":"2023-05-13T16:07:57.594974Z","iopub.status.idle":"2023-05-13T16:07:57.603619Z","shell.execute_reply.started":"2023-05-13T16:07:57.594941Z","shell.execute_reply":"2023-05-13T16:07:57.602598Z"},"trusted":true},"execution_count":175,"outputs":[]},{"cell_type":"code","source":"def eval_model(model, data_loader, loss_fn, device, n_examples):\n  model = model.eval()\n  losses = []\n  correct_predictions = 0\n  with torch.no_grad():\n    for d in data_loader:\n      input_ids = d[\"input_ids\"].to(device)\n      attention_mask = d[\"attention_mask\"].to(device)\n      targets = d[\"targets\"].to(device)\n      outputs = model(\n        input_ids=input_ids,\n        attention_mask=attention_mask\n      )\n      _, preds = torch.max(outputs, dim=1)\n      loss = loss_fn(outputs, targets)\n      correct_predictions += torch.sum(preds == targets)\n      losses.append(loss.item())\n  return correct_predictions.double() / n_examples, np.mean(losses)","metadata":{"execution":{"iopub.status.busy":"2023-05-13T16:07:57.604696Z","iopub.execute_input":"2023-05-13T16:07:57.604980Z","iopub.status.idle":"2023-05-13T16:07:57.617161Z","shell.execute_reply.started":"2023-05-13T16:07:57.604956Z","shell.execute_reply":"2023-05-13T16:07:57.616354Z"},"trusted":true},"execution_count":176,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2023-05-13T16:07:57.618574Z","iopub.execute_input":"2023-05-13T16:07:57.619276Z","iopub.status.idle":"2023-05-13T16:07:57.627948Z","shell.execute_reply.started":"2023-05-13T16:07:57.619243Z","shell.execute_reply":"2023-05-13T16:07:57.626805Z"},"trusted":true},"execution_count":177,"outputs":[]},{"cell_type":"code","source":"#model.load_state_dict(torch.load('best_model_state.bin'))","metadata":{"execution":{"iopub.status.busy":"2023-05-13T16:07:57.629090Z","iopub.execute_input":"2023-05-13T16:07:57.629951Z","iopub.status.idle":"2023-05-13T16:07:58.216210Z","shell.execute_reply.started":"2023-05-13T16:07:57.629916Z","shell.execute_reply":"2023-05-13T16:07:58.214824Z"},"trusted":true},"execution_count":178,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[178], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_model_state.bin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SentimentClassifier:\n\tsize mismatch for out.weight: copying a param with shape torch.Size([3, 768]) from checkpoint, the shape in current model is torch.Size([2, 768]).\n\tsize mismatch for out.bias: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([2])."],"ename":"RuntimeError","evalue":"Error(s) in loading state_dict for SentimentClassifier:\n\tsize mismatch for out.weight: copying a param with shape torch.Size([3, 768]) from checkpoint, the shape in current model is torch.Size([2, 768]).\n\tsize mismatch for out.bias: copying a param with shape torch.Size([3]) from checkpoint, the shape in current model is torch.Size([2]).","output_type":"error"}]},{"cell_type":"code","source":"%%time\nhistory = defaultdict(list)\nbest_accuracy = 0\nfor epoch in range(EPOCHS):\n    print(f'Epoch {epoch + 1}/{EPOCHS}')\n    print('-' * 10)\n    train_acc, train_loss = train_epoch(\n    model,\n    train_data_loader,\n    loss_fn,\n    optimizer,\n    device,\n    scheduler,\n    len(df_train)\n  )\n    print(f'Train loss {train_loss} accuracy {train_acc}')\n    val_acc, val_loss = eval_model(\n    model,\n    val_data_loader,\n    loss_fn,\n    device,\n    len(df_val)\n  )\n    print(f'Val   loss {val_loss} accuracy {val_acc}')\n    print()\n    history['train_acc'].append(train_acc)\n    history['train_loss'].append(train_loss)\n    history['val_acc'].append(val_acc)\n    history['val_loss'].append(val_loss)\n    if val_acc > best_accuracy:\n        torch.save(model.state_dict(), 'best_model_state.bin')\n        best_accuracy = val_acc","metadata":{"execution":{"iopub.status.busy":"2023-05-13T16:09:05.802155Z","iopub.execute_input":"2023-05-13T16:09:05.802551Z","iopub.status.idle":"2023-05-13T16:39:39.705218Z","shell.execute_reply.started":"2023-05-13T16:09:05.802519Z","shell.execute_reply":"2023-05-13T16:39:39.703989Z"},"trusted":true},"execution_count":179,"outputs":[{"name":"stdout","text":"Epoch 1/30\n----------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Train loss 0.5002502043699396 accuracy 0.8580715059588299\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Val   loss 0.4524980102266584 accuracy 0.9708737864077669\n\nEpoch 2/30\n----------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Train loss 0.4801721254299427 accuracy 0.9111592632719393\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Val   loss 0.4385266900062561 accuracy 0.9611650485436892\n\nEpoch 3/30\n----------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Train loss 0.4661971155939431 accuracy 0.9295774647887324\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Val   loss 0.4286335664136069 accuracy 0.9611650485436892\n\nEpoch 4/30\n----------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Train loss 0.46027424212159784 accuracy 0.9355362946912242\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Val   loss 0.4206742175987789 accuracy 0.9611650485436892\n\nEpoch 5/30\n----------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Train loss 0.4511512738877329 accuracy 0.9404117009750812\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Val   loss 0.4143712988921574 accuracy 0.9611650485436892\n\nEpoch 6/30\n----------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Train loss 0.4476298140040759 accuracy 0.9371614301191765\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Val   loss 0.4091076765741621 accuracy 0.9611650485436892\n\nEpoch 7/30\n----------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Train loss 0.43975504487752914 accuracy 0.9442036836403033\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Val   loss 0.4045328370162419 accuracy 0.9611650485436892\n\nEpoch 8/30\n----------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Train loss 0.4367652204016159 accuracy 0.9447453954496208\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Val   loss 0.4007075641836439 accuracy 0.9708737864077669\n\nEpoch 9/30\n----------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Train loss 0.430962139162524 accuracy 0.9452871072589382\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Val   loss 0.3972473612853459 accuracy 0.9708737864077669\n\nEpoch 10/30\n----------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Train loss 0.4287369068840454 accuracy 0.9447453954496208\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Val   loss 0.3941049703529903 accuracy 0.9708737864077669\n\nEpoch 11/30\n----------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Train loss 0.42486299625758467 accuracy 0.9447453954496208\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Val   loss 0.391342431306839 accuracy 0.9708737864077669\n\nEpoch 12/30\n----------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Train loss 0.4246091991662979 accuracy 0.9442036836403033\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Val   loss 0.38889589054243906 accuracy 0.9708737864077669\n\nEpoch 13/30\n----------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Train loss 0.4192938904823928 accuracy 0.9458288190682557\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Val   loss 0.38666865655354093 accuracy 0.9708737864077669\n\nEpoch 14/30\n----------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Train loss 0.41990833318439025 accuracy 0.9420368364030335\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Val   loss 0.38479602336883545 accuracy 0.9708737864077669\n\nEpoch 15/30\n----------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Train loss 0.418301821268838 accuracy 0.9442036836403033\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Val   loss 0.3831628603594644 accuracy 0.9708737864077669\n\nEpoch 16/30\n----------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Train loss 0.41429828133048685 accuracy 0.9431202600216684\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Val   loss 0.38167898995535715 accuracy 0.9708737864077669\n\nEpoch 17/30\n----------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Train loss 0.4127955994215505 accuracy 0.9458288190682557\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Val   loss 0.3803104800837381 accuracy 0.9708737864077669\n\nEpoch 18/30\n----------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Train loss 0.41025788311300604 accuracy 0.9452871072589382\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Val   loss 0.3791402918951852 accuracy 0.9708737864077669\n\nEpoch 19/30\n----------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Train loss 0.410687244914729 accuracy 0.9447453954496208\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Val   loss 0.378028929233551 accuracy 0.9708737864077669\n\nEpoch 20/30\n----------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Train loss 0.41030657291412354 accuracy 0.9436619718309859\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Val   loss 0.3770366098199572 accuracy 0.9708737864077669\n\nEpoch 21/30\n----------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Train loss 0.4083833581414716 accuracy 0.9447453954496208\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Val   loss 0.37614807912281584 accuracy 0.9708737864077669\n\nEpoch 22/30\n----------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Train loss 0.40889964663776857 accuracy 0.9436619718309859\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Val   loss 0.37538359420640127 accuracy 0.9708737864077669\n\nEpoch 23/30\n----------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Train loss 0.4061700349737858 accuracy 0.9452871072589382\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Val   loss 0.37472067560468403 accuracy 0.9708737864077669\n\nEpoch 24/30\n----------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Train loss 0.4077290699913584 accuracy 0.942578548212351\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Val   loss 0.37419591205460684 accuracy 0.9708737864077669\n\nEpoch 25/30\n----------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Train loss 0.4057294425779375 accuracy 0.9447453954496208\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Val   loss 0.3738050290516445 accuracy 0.9708737864077669\n\nEpoch 26/30\n----------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Train loss 0.40488054397804984 accuracy 0.9447453954496208\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Val   loss 0.37350466421672274 accuracy 0.9708737864077669\n\nEpoch 27/30\n----------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Train loss 0.4036520526840769 accuracy 0.9452871072589382\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Val   loss 0.3732724572931017 accuracy 0.9708737864077669\n\nEpoch 28/30\n----------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Train loss 0.40675147520057087 accuracy 0.9414951245937161\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Val   loss 0.3731738669531686 accuracy 0.9708737864077669\n\nEpoch 29/30\n----------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Train loss 0.4036616688144618 accuracy 0.9442036836403033\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Val   loss 0.3731248080730438 accuracy 0.9708737864077669\n\nEpoch 30/30\n----------\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Train loss 0.40235135061987515 accuracy 0.9469122426868906\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"name":"stdout","text":"Val   loss 0.3731171318462917 accuracy 0.9708737864077669\n\nCPU times: user 29min 41s, sys: 34.4 s, total: 30min 15s\nWall time: 30min 33s\n","output_type":"stream"}]},{"cell_type":"code","source":"test_acc, _ = eval_model(\n  model,\n  test_data_loader,\n  loss_fn,\n  device,\n  len(df_test)\n)\ntest_acc.item()","metadata":{"execution":{"iopub.status.busy":"2023-05-13T16:49:04.979182Z","iopub.execute_input":"2023-05-13T16:49:04.980586Z","iopub.status.idle":"2023-05-13T16:49:06.373344Z","shell.execute_reply.started":"2023-05-13T16:49:04.980529Z","shell.execute_reply":"2023-05-13T16:49:06.372098Z"},"trusted":true},"execution_count":180,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"},{"execution_count":180,"output_type":"execute_result","data":{"text/plain":"0.9223300970873786"},"metadata":{}}]},{"cell_type":"code","source":"def get_predictions(model, data_loader):\n  model = model.eval()\n  review_texts = []\n  predictions = []\n  prediction_probs = []\n  real_values = []\n  with torch.no_grad():\n    for d in data_loader:\n      texts = d[\"sentence_text\"]\n      input_ids = d[\"input_ids\"].to(device)\n      attention_mask = d[\"attention_mask\"].to(device)\n      targets = d[\"targets\"].to(device)\n      outputs = model(\n        input_ids=input_ids,\n        attention_mask=attention_mask\n      )\n      _, preds = torch.max(outputs, dim=1)\n      review_texts.extend(texts)\n      predictions.extend(preds)\n      prediction_probs.extend(outputs)\n      real_values.extend(targets)\n  predictions = torch.stack(predictions).cpu()\n  prediction_probs = torch.stack(prediction_probs).cpu()\n  real_values = torch.stack(real_values).cpu()\n  return review_texts, predictions, prediction_probs, real_values","metadata":{"execution":{"iopub.status.busy":"2023-05-13T16:49:06.376051Z","iopub.execute_input":"2023-05-13T16:49:06.376811Z","iopub.status.idle":"2023-05-13T16:49:06.388853Z","shell.execute_reply.started":"2023-05-13T16:49:06.376767Z","shell.execute_reply":"2023-05-13T16:49:06.387767Z"},"trusted":true},"execution_count":181,"outputs":[]},{"cell_type":"code","source":"y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n  model,\n  test_data_loader\n)","metadata":{"execution":{"iopub.status.busy":"2023-05-13T16:49:06.390462Z","iopub.execute_input":"2023-05-13T16:49:06.391210Z","iopub.status.idle":"2023-05-13T16:49:07.757118Z","shell.execute_reply.started":"2023-05-13T16:49:06.391177Z","shell.execute_reply":"2023-05-13T16:49:07.755450Z"},"trusted":true},"execution_count":182,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2354: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/tmp/ipykernel_31/3943356482.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  input_ids = torch.tensor(encoding['input_ids'], dtype=torch.long)\n/tmp/ipykernel_31/3943356482.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  attention_mask = torch.tensor(encoding['attention_mask'], dtype=torch.long)\n","output_type":"stream"}]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred, target_names=['négative','positif']))","metadata":{"execution":{"iopub.status.busy":"2023-05-13T16:49:07.761582Z","iopub.execute_input":"2023-05-13T16:49:07.764420Z","iopub.status.idle":"2023-05-13T16:49:07.787598Z","shell.execute_reply.started":"2023-05-13T16:49:07.764369Z","shell.execute_reply":"2023-05-13T16:49:07.786564Z"},"trusted":true},"execution_count":183,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n    négative       0.87      0.98      0.92        48\n     positif       0.98      0.87      0.92        55\n\n    accuracy                           0.92       103\n   macro avg       0.92      0.93      0.92       103\nweighted avg       0.93      0.92      0.92       103\n\n","output_type":"stream"}]},{"cell_type":"code","source":"def show_confusion_matrix(confusion_matrix):\n  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n  plt.ylabel('True sentiment')\n  plt.xlabel('Predicted sentiment');\ncm = confusion_matrix(y_test, y_pred)\ndf_cm = pd.DataFrame(cm, index=class_values, columns=['négative','positif'])\nshow_confusion_matrix(df_cm)","metadata":{"execution":{"iopub.status.busy":"2023-05-13T16:49:07.791721Z","iopub.execute_input":"2023-05-13T16:49:07.793983Z","iopub.status.idle":"2023-05-13T16:49:08.187362Z","shell.execute_reply.started":"2023-05-13T16:49:07.793941Z","shell.execute_reply":"2023-05-13T16:49:08.186473Z"},"trusted":true},"execution_count":184,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 2 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"idx = 2\nreview_text = y_review_texts[idx]\ntrue_sentiment = y_test[idx]\npred_df = pd.DataFrame({\n  'class_values': class_values,\n  'values': y_pred_probs[idx]\n})\nprint(\"\\n\".join(wrap(review_text)))\nprint()\nprint(f'True sentiment: {class_values[true_sentiment]}')","metadata":{"execution":{"iopub.status.busy":"2023-05-13T16:49:08.191396Z","iopub.execute_input":"2023-05-13T16:49:08.193537Z","iopub.status.idle":"2023-05-13T16:49:08.205522Z","shell.execute_reply.started":"2023-05-13T16:49:08.193501Z","shell.execute_reply":"2023-05-13T16:49:08.204605Z"},"trusted":true},"execution_count":185,"outputs":[{"name":"stdout","text":" أوقفوها و اريحونا من برامجها و الضرائب التي ندفعها لها ظلما و عدونا\nرغما عنا\n\nTrue sentiment: 0\n","output_type":"stream"}]},{"cell_type":"code","source":"sample_txt= \"had lhotelat ma mori7inch\"\nsample_txt_ar=transliterate(sample_txt, source='ma', target='ar', universal=True)\nencoded_sentence = DarijaBERT_tokenizer.encode_plus(\n  sample_txt_ar,\n  max_length=MAX_LEN,\n  add_special_tokens=True,\n  return_token_type_ids=False,\n  pad_to_max_length=True,\n  return_attention_mask=True,\n  return_tensors='pt',\n)","metadata":{"execution":{"iopub.status.busy":"2023-05-13T16:51:26.077216Z","iopub.execute_input":"2023-05-13T16:51:26.077709Z","iopub.status.idle":"2023-05-13T16:51:26.086313Z","shell.execute_reply.started":"2023-05-13T16:51:26.077662Z","shell.execute_reply":"2023-05-13T16:51:26.084792Z"},"trusted":true},"execution_count":190,"outputs":[]},{"cell_type":"code","source":"input_ids = encoded_sentence['input_ids'].to(device)\nattention_mask = encoded_sentence['attention_mask'].to(device)\noutput = model(input_ids, attention_mask)\n_, prediction = torch.max(output, dim=1)\nprint(f'Review text: {sample_txt_ar}')\nif class_values[prediction]==1:\n    print('positif')\nelse:\n    print('négatif')\n##print(f'Sentiment  : {class_values[prediction]}')","metadata":{"execution":{"iopub.status.busy":"2023-05-13T16:51:38.139836Z","iopub.execute_input":"2023-05-13T16:51:38.140638Z","iopub.status.idle":"2023-05-13T16:51:38.171948Z","shell.execute_reply.started":"2023-05-13T16:51:38.140600Z","shell.execute_reply":"2023-05-13T16:51:38.170820Z"},"trusted":true},"execution_count":191,"outputs":[{"name":"stdout","text":"Review text: هاد لهوتلات ما موريحينش\nnégatif\n","output_type":"stream"}]}]}
